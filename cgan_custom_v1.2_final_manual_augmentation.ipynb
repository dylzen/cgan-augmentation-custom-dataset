{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "pyplot per i grafici\n",
    "\n",
    "numpy per la gestione degli array/tensori\n",
    "\n",
    "os, glob, pathlib per gestione filesystem/cartelle/files\n",
    "\n",
    "wandb per il tracking degli esperimenti\n",
    "\n",
    "pandas per il dataframe proveniente dal .csv\n",
    "\n",
    "time per aggiungere il timestamp al file del modello e delle immagini sintetiche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definizione iperparametri\n",
    "Frutto di lungo processo di trial/error. I valori dei learning rates utilizzati (per l'algoritmo Adam) evitano che le funzioni di costo di discriminator e generator non divergano dopo poche decine di epochs, consentendo un training stabile per oltre 1500 epochs (~2h su Colab, 4h sul mio hardware)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8          # default tutorial MNIST = 64, default Gennaro = 5\n",
    "num_channels = 1\n",
    "num_classes = 2         # default MNIST = 10 (cifre 0-9), ora Healthy o Patient -> 2\n",
    "image_size = 64         # i .png del dataset sono 64x64\n",
    "latent_dim = 128        # provato 256 per ovviare al mode collapse, poca differenza\n",
    "\n",
    "lr_disc = 0.0001       \n",
    "lr_gen = 0.0001        # default ADAM = 0.001, default tutorial MNIST = 0.0003, divergono (g_loss altissima)\n",
    "decay_disc = 0.5        # default = 0.9, provato 0.5 ma diverge\n",
    "decay_gen = 0.5         # default = 0.9, provato 0.5 ma diverge\n",
    "n_epochs = 100         \n",
    "lbl_smooth = 0          \n",
    "# patience = 200        # valore per l'EarlyStopping usato per i primi addedstramenti\n",
    "loss_func = \"binary_crossentropy\"\n",
    "nl = '01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Controllo percorsi e visualizzo immagini del dataset dal filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = str(pathlib.Path.home())\n",
    "dataset_dir = pathlib.PurePath(home_path, 'Documents', 'Neural Networks', 'datasets')\n",
    "print(f'Cartella dei datasets:', dataset_dir)\n",
    "\n",
    "seed_name = \"meander02\"\n",
    "src_path_train = pathlib.PurePath(dataset_dir, 'accXaccY_transformed', '64', '50_50', 'meander', '02', 'training')\n",
    "print(f'Cartella del dataset di training', src_path_train)\n",
    "\n",
    "sub_class = [f for f in os.listdir(src_path_train) if not f.startswith('.')]\n",
    "print(f'Labels trovate nei dati di training:', sub_class)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "path = os.path.join(src_path_train,sub_class[0])\n",
    "\n",
    "filelist = glob.glob(path+'/*')\n",
    "\n",
    "for i in range(4):\n",
    "    print(filelist[i])\n",
    "    plt.subplot(240 + 1 + i)\n",
    "    img = plt.imread(filelist[i])\n",
    "    plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "src_path_test = pathlib.PurePath(dataset_dir, 'accXaccY_transformed', '64', '50_50', 'meander', '02', 'test')\n",
    "print(f'\\nCartella del dataset di test' ,src_path_test)\n",
    "\n",
    "sub_class = [f for f in os.listdir(src_path_test) if not f.startswith('.')]\n",
    "print(f'Labels trovate nei dati di test:', sub_class)\n",
    "\n",
    "path = os.path.join(src_path_test, sub_class[1])\n",
    "\n",
    "filelist = glob.glob(path+'/*')\n",
    "for i in range(4,8):\n",
    "    print(filelist[i])\n",
    "    plt.subplot(240 + 1 + i)\n",
    "    img = plt.imread(filelist[i])\n",
    "    plt.imshow(img, cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Carico il dataset tramite il metodo di keras 'image_dataset_from_directory'\n",
    "Il metodo genera un oggetto \"tf.data.Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=src_path_train,\n",
    "    validation_split=0.3,                   # stesso usato da Gennaro\n",
    "    subset='training',                      # divisione del set\n",
    "    labels='inferred',                      # prende i modi delle classi dalle sottocartelle del set\n",
    "    label_mode='categorical',               # labels sono codificate come categorical vector con valori binari (sostituisce il metodo utils.to_categorical)\n",
    "    class_names=['H', 'P'],                 # devono corrispondere ai nomi delle sottocartelle\n",
    "    color_mode='grayscale',                 # canale dei colori\n",
    "    batch_size=batch_size,                  \n",
    "    image_size=(image_size, image_size),\n",
    "    shuffle=False,                          # uso il seed originale, senza modificare l'ordine dei samples\n",
    "    # seed=123\n",
    ")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=src_path_train,\n",
    "    validation_split=0.3,\n",
    "    subset='validation',\n",
    "    image_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    labels=\"inferred\",\n",
    "    label_mode='categorical',\n",
    "    class_names=['H', 'P'],\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=False,\n",
    "    # seed=123\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = src_path_test,\n",
    "    labels = \"inferred\",\n",
    "    label_mode='categorical',\n",
    "    class_names = ['H', 'P'],\n",
    "    color_mode = \"grayscale\",\n",
    "    batch_size = 1,                         # uso un singolo sample come batch per la fase di test\n",
    "    image_size = (image_size, image_size),\n",
    "    shuffle = False,\n",
    "    # seed=123\n",
    ")\n",
    "\n",
    "# Verifico corretto caricamento dei diversi sets e delle labels\n",
    "class_names_train = train_ds.class_names\n",
    "print(f'Lista delle classi (train): {class_names_train}')\n",
    "\n",
    "class_names_test = test_ds.class_names\n",
    "print(f'Lista delle classi (test): {class_names_test}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_classes = len(class_names_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Salvo numero di samples per ogni set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [item for item in os.listdir(src_path_train) if os.path.isdir(os.path.join(src_path_train, item))]\n",
    "nb_train_samples = ( sum([len(files) for _, _, files in os.walk(src_path_train)]) - 1 )\n",
    "nb_validation_samples = round(nb_train_samples * 0.3) - 1\n",
    "validation_steps = nb_validation_samples / batch_size\n",
    "nb_synth_samples = 0\n",
    "nb_test_samples = ( sum([len(files) for _, _, files in os.walk(src_path_test)]) - 1)\n",
    "total_nb_samples = nb_train_samples + nb_test_samples\n",
    "nb_classes = len(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_ds))\n",
    "image = image[0, :, :, :]\n",
    "_ = plt.imshow(np.squeeze(image))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizzo immagini dopo aver caricato il dataset\n",
    "Mi assicuro che il dataset venga caricato correttamente, con le labels come titolo\n",
    "\n",
    "[1. 0.] = Healthy\n",
    "\n",
    "[0. 1.] = Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in train_ds.shuffle(buffer_size=100).take(1):\n",
    "  for i in range(5):\n",
    "    ax = plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"), cmap=plt.get_cmap('gray'))\n",
    "    plt.title(np.array(labels[i]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_ds.shuffle(buffer_size=100).take(1):\n",
    "  for i in range(1):\n",
    "    ax = plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"), cmap=plt.get_cmap('gray'))\n",
    "    plt.title(np.array(labels[i]))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verifica shapes di immagini e labels\n",
    "mi deve restituire:\n",
    "\n",
    "(batch_size, 64, 64, 1) per le immagini\n",
    "\n",
    "(batch_size, 2) per le labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(f'Shape delle immagini di training (batch):', image_batch.shape)\n",
    "  # print(image_batch[0])\n",
    "  print(f'Shape delle labels di training (batch):', labels_batch.shape)\n",
    "  print(labels_batch[0])\n",
    "  break\n",
    "\n",
    "for image_batch, labels_batch in test_ds:\n",
    "  print(f'Shape delle immagini di test (batch):', image_batch.shape)\n",
    "  # print(image_batch[0])\n",
    "  print(f'Shape delle labels di test (batch):', labels_batch.shape)\n",
    "  print(labels_batch[0])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_ds))\n",
    "image = image[0, :, :, :]\n",
    "_ = plt.imshow(image, cmap='gray')\n",
    "print(image.shape)\n",
    "\n",
    "# _ = plt.title(get_label_name(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_rescale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  # image = tf.image.resize(image, [image_size, image_size])\n",
    "  image = (image / 255.0)\n",
    "  return image, label\n",
    "\n",
    "def augment_0(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  \n",
    "  return image, label\n",
    "\n",
    "def augment_6(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.flip_left_right(image)\n",
    "  image = tf.image.flip_up_down(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_1(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_7(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image)\n",
    "  image = tf.image.flip_left_right(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_8(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image)\n",
    "  image = tf.image.flip_up_down(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_9(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image)\n",
    "  image = tf.image.flip_up_down(image)\n",
    "  image = tf.image.flip_left_right(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_2(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.flip_left_right(image)\n",
    " \n",
    "  return image, label\n",
    "\n",
    "def augment_3(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.flip_up_down(image)\n",
    " \n",
    "  return image, label\n",
    "\n",
    "def augment_4(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image, k=2)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_10(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image, k=2)\n",
    "  image = tf.image.flip_up_down(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_11(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image, k=2)\n",
    "  image = tf.image.flip_left_right(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_12(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image, k=2)\n",
    "  image = tf.image.flip_left_right(image)\n",
    "  image = tf.image.flip_up_down(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_5(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image, k=3)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_13(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image, k=3)\n",
    "  image = tf.image.flip_left_right(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_14(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image, k=3)\n",
    "  image = tf.image.flip_up_down(image)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "def augment_15(image, label):\n",
    "  image, label = resize_and_rescale(image, label)\n",
    "  image = tf.image.rot90(image, k=3)\n",
    "  image = tf.image.flip_left_right(image)\n",
    "  image = tf.image.flip_up_down(image)\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# Dataset originale\n",
    "normalized_train_ds_0 = (\n",
    "    train_ds\n",
    "    .map(augment_0, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_1 = (\n",
    "    train_ds\n",
    "    .map(augment_1, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_2 = (\n",
    "    train_ds\n",
    "    .map(augment_2, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_3 = (\n",
    "    train_ds\n",
    "    .map(augment_3, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_4 = (\n",
    "    train_ds\n",
    "    .map(augment_4, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_5 = (\n",
    "    train_ds\n",
    "    .map(augment_5, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_6 = (\n",
    "    train_ds\n",
    "    .map(augment_6, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_7 = (\n",
    "    train_ds\n",
    "    .map(augment_7, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_8 = (\n",
    "    train_ds\n",
    "    .map(augment_8, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_9 = (\n",
    "    train_ds\n",
    "    .map(augment_9, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_10 = (\n",
    "    train_ds\n",
    "    .map(augment_10, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_11 = (\n",
    "    train_ds\n",
    "    .map(augment_11, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_12 = (\n",
    "    train_ds\n",
    "    .map(augment_12, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_13 = (\n",
    "    train_ds\n",
    "    .map(augment_13, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_14 = (\n",
    "    train_ds\n",
    "    .map(augment_14, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "# Dataset aumentato\n",
    "normalized_train_ds_15 = (\n",
    "    train_ds\n",
    "    .map(augment_15, num_parallel_calls=AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15), frameon=False)\n",
    "plt.subplot(1, 5, i + 1, frameon=False)\n",
    "for images, labels in normalized_train_ds_12.shuffle(buffer_size=100).take(1):\n",
    "  for i in range(3):\n",
    "    \n",
    "       \n",
    "    plt.imshow(np.squeeze(images[i].numpy()), cmap=plt.get_cmap('gray'))\n",
    "    plt.title(np.array(labels[i]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('augmented'+str(i)+'.png', format='png', bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_ds = normalized_train_ds_0.concatenate(normalized_train_ds_1).concatenate(normalized_train_ds_2).concatenate(normalized_train_ds_3).concatenate(normalized_train_ds_4).concatenate(normalized_train_ds_5).concatenate(normalized_train_ds_6).concatenate(normalized_train_ds_7).concatenate(normalized_train_ds_8).concatenate(normalized_train_ds_9).concatenate(normalized_train_ds_10).concatenate(normalized_train_ds_11).concatenate(normalized_train_ds_12).concatenate(normalized_train_ds_13).concatenate(normalized_train_ds_14).concatenate(normalized_train_ds_15)\n",
    "\n",
    "full_train_ds = (\n",
    "    full_train_ds\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "full_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_val_ds = (\n",
    "    val_ds\n",
    "    .map(resize_and_rescale, num_parallel_calls=AUTOTUNE)\n",
    "    # .batch(batch_size)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "normalized_test_ds = (\n",
    "    test_ds\n",
    "    .map(resize_and_rescale, num_parallel_calls=AUTOTUNE)\n",
    "    # .batch(batch_size)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in full_train_ds:\n",
    "  print(f'Shape delle immagini di training (batch):', image_batch.shape)\n",
    "  # print(image_batch[0])\n",
    "  print(f'Shape delle labels di training (batch):', labels_batch.shape)\n",
    "  print(labels_batch[0])\n",
    "  break\n",
    "\n",
    "for image_batch, labels_batch in normalized_test_ds:\n",
    "  print(f'Shape delle immagini di test (batch):', image_batch.shape)\n",
    "  # print(image_batch[0])\n",
    "  print(f'Shape delle labels di test (batch):', labels_batch.shape)\n",
    "  print(labels_batch[0])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_ds_image_batch, norm_train_ds_labels_batch = next(iter(full_train_ds))\n",
    "first_image = norm_train_ds_image_batch[0]\n",
    "first_label = norm_train_ds_labels_batch[0]\n",
    "# Testo che i valori siano effettivamente in [0,1]\n",
    "print(\"Intervallo train set: \", np.min(first_image), np.max(first_image))\n",
    "print(\"Intervallo labels: \", np.min(first_label), np.max(first_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method is to simply increase the size of your training set. This method consists in loading an image into memory, modifying it, taking the transformed image and the original image, and saving them both in the training file. This method can be useful but does not really help our model to generalize.\n",
    "\n",
    "The most widespread method is the second method also called On The Fly Data Augmentation [1]. It is this method that the ImageDataGenerator class implements. In this case, the model does not train on the initial instance but only on the generated instance which changes at each epoch to allow the network to better generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length_orig = [i for i,_ in enumerate(train_ds)][-1] + 1\n",
    "dataset_length_orig = dataset_length_orig * batch_size\n",
    "print(f\"Campioni dataset originale:\", dataset_length_orig)\n",
    "\n",
    "dataset_length_aug = [i for i,_ in enumerate(full_train_ds)][-1] + 1\n",
    "dataset_length_aug = dataset_length_aug * batch_size\n",
    "print(f\"Campioni dataset aumentato:\", dataset_length_aug)\n",
    "\n",
    "nb_train_samples = dataset_length_aug\n",
    "\n",
    "augmented_samples = dataset_length_aug - dataset_length_orig\n",
    "\n",
    "target_names = [item for item in os.listdir(src_path_train) if os.path.isdir(os.path.join(src_path_train, item))]\n",
    "# nb_train_samples = ( sum([len(files) for _, _, files in os.walk(src_path_train)]) - 1 )\n",
    "nb_validation_samples = round(dataset_length_orig * 0.3) - 1\n",
    "validation_steps = nb_validation_samples / batch_size\n",
    "nb_synth_samples = 0\n",
    "nb_test_samples = ( sum([len(files) for _, _, files in os.walk(src_path_test)]) - 1)\n",
    "total_nb_samples = nb_train_samples + nb_test_samples\n",
    "nb_classes = len(target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definisco numero di canali in input alle due reti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_in_channels = latent_dim + num_classes        # 128 punti dello spazio + numero classi (H/P)\n",
    "discriminator_in_channels = num_channels + num_classes  # numero di canali (1=grayscale) + num classi (H/P)\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definizione modelli\n",
    "Sia Generatore che Discriminatore sono definite tramite le Sequential API di Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)         # best practice per stabilizzare il training di una DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generatore e Discriminatore\n",
    "\n",
    "Implementazione DCGAN\n",
    "\n",
    "https://keras.io/examples/generative/dcgan_overriding_train_step/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "discriminator = tf.keras.Sequential(\n",
    "    [\n",
    "        # L'input al generatore è un sample (reale o sintetico) con shape (64,64,num_canali+num_classi)\n",
    "        tf.keras.layers.InputLayer(input_shape=(64, 64, discriminator_in_channels)),\n",
    "        # (64,64,3) -> (32,32,64) = downsampling\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2), # best practice\n",
    "        # (32,32,64) -> (16,16,128) = downsampling\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        # (16,16,128) -> (8,8,128) = downsampling\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        # (8,8,128) -> prediction (fake/real)\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "# Mi assicuro che l'output abbia la giusta shape\n",
    "assert discriminator.output_shape == (None, 1)\n",
    "discriminator.summary()\n",
    "\n",
    "generator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer(input_shape=(generator_in_channels,)),\n",
    "        # Vogliamo generare latent_dim(128) + num_classes coefficienti per ricostruirli in una map da\n",
    "        # 8x8x(128 + num_classes).\n",
    "        tf.keras.layers.Dense(8 * 8 * generator_in_channels),\n",
    "        tf.keras.layers.Reshape((8, 8, generator_in_channels)),\n",
    "        # kernel deve avere dim multiple dello stride (no checkerboard effect)\n",
    "        # (8,8,latent_dim+n_classes) -> (16,16,128) = upsampling  \n",
    "        tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),  # best practice\n",
    "        # (16,16,128) -> (32,32,256) = upsampling\n",
    "        tf.keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        # (32,32,256) -> (64,64,512) = upsampling\n",
    "        tf.keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        # (64,64,512) -> (64,64,1) = output, immagine 64x64 greyscale\n",
    "        tf.keras.layers.Conv2D(1, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "# Mi assicuro che l'output abbia la giusta shape\n",
    "assert generator.output_shape == (None, 64, 64, 1)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uso il generatore non addestrato per generare un'immagine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([2,generator_in_channels])\n",
    "generated_image = generator(noise, training=False)\n",
    "print(generated_image.shape)\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model subclassing e override dei metodi train_step e test_step\n",
    "\n",
    "Implementazione CGAN\n",
    "\n",
    "https://keras.io/examples/generative/conditional_gan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConditionalGAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"d_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker, self.train_mse_metric, self.train_acc_metric]   # aggiunto le due metriche custom (MSE e Accuracy)\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn, train_mse_metric, train_acc_metric):                   # aggiunto le due metriche custom (MSE e Accuracy)\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_mse_metric = train_mse_metric # MSE\n",
    "        self.train_acc_metric = train_acc_metric # BinaryAccuracy\n",
    "    \n",
    "    # Le sezioni commentate in inglese sono quelle di default e non sono state modificate   \n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "        \n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights))\n",
    "        \n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        \n",
    "        # Aggiungo le metriche richieste\n",
    "        self.train_acc_metric.update_state(labels, self.discriminator(combined_images, training=True))\n",
    "        self.train_mse_metric.update_state(labels, self.discriminator(combined_images, training=True))\n",
    "        # Restituisco tutte le metriche\n",
    "        return { m.name: m.result() for m in self.metrics }\n",
    "    \n",
    "    # override del test step per implementazione di model.evaluate e aggiunta metriche\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "        \n",
    "        # Da qui in poi aggiungo le loss e le metriche evitando backpropagation e training\n",
    "        val_d_logits = self.discriminator(combined_images, training=False)\n",
    "        val_g_logits = self.discriminator(fake_image_and_labels, training=False)\n",
    "\n",
    "        # Compute the loss value\n",
    "        self.val_d_loss_value = self.loss_fn(labels, val_d_logits)\n",
    "        self.val_g_loss_value = self.loss_fn(misleading_labels, val_g_logits)\n",
    "        \n",
    "        # Monitor loss.\n",
    "        self.disc_loss_tracker.update_state(self.val_d_loss_value)\n",
    "        self.gen_loss_tracker.update_state(self.val_g_loss_value)\n",
    "        # Aggiungo le metriche richieste\n",
    "        self.train_acc_metric.update_state(labels, self.discriminator(combined_images, training=False))\n",
    "        self.train_mse_metric.update_state(labels, self.discriminator(combined_images, training=False))\n",
    "        \n",
    "        # Restituisco tutte le metriche\n",
    "        return { m.name: m.result() for m in self.metrics }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging e tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definizione dei Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'logs')\n",
    "csv_dir = os.path.join(log_dir, 'csv')\n",
    "model_checkpoints_dir = os.path.join(log_dir, 'model_checkpoints')\n",
    "model_checkpoints_timestamp = pathlib.Path(model_checkpoints_dir, timestr)\n",
    "model_checkpoints_timestamp.mkdir(parents=True, exist_ok=True)\n",
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = pathlib.Path(model_checkpoints_timestamp, \"cp-epo{epoch:04d}-g_loss{g_loss:.2f}-d_loss{d_loss:.2f}-acc{accuracy:.2f}-mse{mse:.2f}\")\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "print(checkpoint_path)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "csv_file = csv_dir + '/training_results_' + str(seed_name) + \"_epoch\" + str(n_epochs) + \"_\" + str(timestr) + '.csv'\n",
    "print(csv_file)\n",
    "\n",
    "def get_callbacks(log_dir):\n",
    "  return [\n",
    "    # tf.keras.callbacks.EarlyStopping(   # usato solo inizialmente quando le loss divergevano subito\n",
    "    #   monitor='g_loss',\n",
    "    #   patience=patience,\n",
    "    #   verbose=1,\n",
    "    #   restore_best_weights=True,\n",
    "    # ),\n",
    "    tf.keras.callbacks.CSVLogger(csv_file),   # costruisco un file csv con tutte le metriche\n",
    "    tf.keras.callbacks.ModelCheckpoint(       # per salvare i modelli (circa 50MB per checkpoint). Salvo solo weights perchè non posso salvare l'intero modello con subclassing\n",
    "      filepath=checkpoint_path,\n",
    "      # monitor='val_g_loss',\n",
    "      verbose=1,\n",
    "      save_best_only=False,\n",
    "      save_weights_only=True,\n",
    "      save_freq='epoch',\n",
    "      period=50,\n",
    "    )\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom callback per la generazione di immagini durante il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_dir = \"logs/generated_images/training\"\n",
    "timestamp_dir = pathlib.Path(generated_images_dir, timestr)\n",
    "timestamp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=generator_in_channels):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # start = time.time()\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                img.save(os.path.join(timestamp_dir, \"synth_{}_epo{}_{}.png\".format(i, epoch+1, timestr)))\n",
    "                print(\"\\nSalvata immagine con nome synth_{}_epo{}_{}.png\".format(i, epoch+1, timestr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inizializzo parametri wandb\n",
    "\n",
    "Usati per il logging sul sito utilizzato per tracciare gli esperimenti. Non rilevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"wandb_projectname\", entity=\"wandb_username\",\n",
    "        config= {\n",
    "                \"dataset\": seed_name,\n",
    "                \"train_samples\": nb_train_samples,\n",
    "                \"validation_samples\": nb_validation_samples,\n",
    "                \"synth_samples\": nb_synth_samples,\n",
    "                \"test_samples\": nb_test_samples,\n",
    "                \"latent_dim\": latent_dim,\n",
    "                \"lr_discr\": lr_disc,\n",
    "                \"decay_discr\": decay_disc,\n",
    "                \"lr_gen\": lr_gen,\n",
    "                \"decay_gen\": decay_gen,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"norm\": nl,\n",
    "                \"loss_func\": loss_func,\n",
    "                \"label_smoothing\": lbl_smooth,\n",
    "                }\n",
    ")\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=lr_disc, beta_1=decay_disc), \n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=lr_gen, beta_1=decay_gen),   \n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
    "    train_mse_metric=tf.keras.metrics.MeanSquaredError(name='mse'),                 # aggiunto\n",
    "    train_acc_metric=tf.keras.metrics.BinaryAccuracy(name='accuracy')               # aggiunto\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stampo riepilogo degli iperparametri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non rilevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training di una C-GAN ...')\n",
    "print(f'- dataset: ', seed_name)\n",
    "print(f'- lista delle classi: ', class_names_train)\n",
    "print(f'- # di classi: ', n_classes)\n",
    "print(f'- # di campioni per il training: ', nb_train_samples)\n",
    "print(f'- # di campioni per la validation: ', nb_validation_samples)\n",
    "print(f'- # di campioni per il test: ', nb_test_samples)\n",
    "print(f'- # totale di campioni: ', total_nb_samples) \n",
    "print(f'- percentuale di campioni per il training: ', round(nb_train_samples/total_nb_samples*100, 2), '%')\n",
    "print(f'- percentuale di campioni per la validation: ', round(nb_validation_samples/nb_train_samples*100), '%')\n",
    "print(f'- percentuale di campioni per il test: ', round(nb_test_samples/total_nb_samples*100, 2), ' %')\n",
    "print(f'- dimensioni dello spazio latente: ', latent_dim)\n",
    "print(f'- # di epochs: ', n_epochs - 1)\n",
    "print(f'- diemnsioni dei batch: ', batch_size)\n",
    "print(f'- learning rate disc.: ', lr_disc)\n",
    "print(f'- learning rate gen.: ', lr_gen)\n",
    "print(f'- normalizzazione: ', nl)\n",
    "print(f'- # di validation steps: ', round(validation_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inizio training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cond_gan.fit(full_train_ds, epochs=n_epochs, validation_data=(normalized_val_ds), validation_steps=3, callbacks=[GANMonitor(num_img=5), get_callbacks(log_dir=log_dir), WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Salvo il modello del generatore finale per utilizzo futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_generator = cond_gan.generator\n",
    "# trained_generator.save(\"models/cgan_gen_\" + str(seed_name) + \"_epoch\" + str(n_epochs) + \"_\" + str(timestr) + \".h5\")\n",
    "# print(\"Modello salvato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fase di test\n",
    "Uso il metodo model.evaluate() per avere le metriche corrispondenti al test dataset.\n",
    "\n",
    "Ciò è possibile solo dopo l'override del train_step del metodo model.fit(), altrimenti restituisce errore per il subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = cond_gan.evaluate(normalized_test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stampo le metriche come dataframe\n",
    "(nel caso non volessi utilizzare wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<15} {:<10}\".format('Metric','Value'))\n",
    "metric_value = []\n",
    "for k, v in evaluate.items():\n",
    "    metric, value = k, v\n",
    "    print(\"{:<15} {:<10}\".format(\"test_\"+metric, value))\n",
    "    metric_value.append(evaluate.get(metric))\n",
    "print(metric_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggiungo le metriche del test dataset al csv generato dal callback\n",
    "(nel caso non volessi utilizzare wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file)\n",
    "df['test_accuracy'] = metric_value[3]\n",
    "df['test_mse'] = metric_value[2]\n",
    "df['test_d_loss'] = metric_value[1]\n",
    "df['test_g_loss'] = metric_value[0]\n",
    "df['dataset_name'] = seed_name\n",
    "df.to_csv(csv_file)\n",
    "last_row = df.iloc[-1]\n",
    "print(last_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Termino il logging su wandb\n",
    "1) Logging manuale delle metriche di test\n",
    "2) Invio email al termine del training\n",
    "3) Termino la run e salvo i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"test_g_loss\": metric_value[0], \"test_d_loss\": metric_value[1], \"test_mse\": metric_value[2], \"test_accuracy\": metric_value[3]})\n",
    "wandb.alert(title=\"Run finished\", text=\"Run finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caricamento weights da un'eventuale checkpoint e successiva compilazione per avere modello completo.\n",
    "Il file del modello verrà utilizzato nello script per la generazione delle immagini \"image_generation.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico checkpoint modello 20220521-162822\n",
    "timestamp = \"20220507-145052\"\n",
    "weight_file = \"logs/model_checkpoints/20220507-145052 mea02/cp-epo0040-g_loss0.78-d_loss0.72-acc0.59-mse0.24\"\n",
    "cp_name = \"cp-epo0040-g_loss0.78-d_loss0.72-acc0.59-mse0.24\"\n",
    "cond_gan.load_weights(weight_file).expect_partial()\n",
    "\n",
    "print(\"Pesi caricati con successo. Modello pronto per essere compilato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trained_generator = cond_gan.generator\n",
    "best_trained_generator.compile()\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_trained_generator.save(\"models/best_augmented_generator_\" + str(timestamp) + \"_\" + cp_name + \".h5\")\n",
    "print(\"Modello migliore salvato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trained_generator = cond_gan.generator\n",
    "best_trained_generator.compile()\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_trained_generator.save(\"models/best_not_augmented_generator_\" + str(timestamp) + \"_\" + cp_name + \".h5\")\n",
    "print(\"Modello migliore salvato.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c47b1065398c2163c640202fad4fcf79d15a59777db3d8c704111e37697c1cab"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
